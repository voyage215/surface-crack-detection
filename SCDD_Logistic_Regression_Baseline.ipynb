{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCDD-Logistic-Regression-Baseline.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OkleH_OMFaZ"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR81LcTEjsRg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nUN9L0YdvDZ"
      },
      "source": [
        "!cp '/content/drive/My Drive/data.csv' ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naJiNM8IeA9k"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "iyXGblZtgQ-k",
        "outputId": "1dfc4225-f6ed-45f1-f7a8-1c3db0e34baa"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.713726</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>...</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.803922</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.803922</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.898039</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>...</td>\n",
              "      <td>0.886275</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.886275</td>\n",
              "      <td>0.905882</td>\n",
              "      <td>0.917647</td>\n",
              "      <td>0.803922</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.835294</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.662745</td>\n",
              "      <td>0.650980</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.721569</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.643137</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.698039</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.635294</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.556863</td>\n",
              "      <td>0.549020</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.631373</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.631373</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.713726</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.698039</td>\n",
              "      <td>0.635294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.713726</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.698039</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.839216</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.639216</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.854902</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.713726</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.749020</td>\n",
              "      <td>0.721569</td>\n",
              "      <td>...</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.721569</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.737255</td>\n",
              "      <td>0.737255</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.698039</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.639216</td>\n",
              "      <td>0.635294</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.803922</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.749020</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.803922</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.749020</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.768627</td>\n",
              "      <td>0.749020</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.792157</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.749020</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.788235</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.737255</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 2502 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         0         1  ...      2498      2499  target\n",
              "0           0  0.823529  0.831373  ...  0.819608  0.807843       0\n",
              "1           1  0.874510  0.858824  ...  0.815686  0.839216       0\n",
              "2           2  0.701961  0.674510  ...  0.698039  0.635294       0\n",
              "3           3  0.729412  0.729412  ...  0.635294  0.705882       0\n",
              "4           4  0.796078  0.788235  ...  0.741176  0.729412       0\n",
              "\n",
              "[5 rows x 2502 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUCqkxUXg8oO"
      },
      "source": [
        "data = data.drop('Unnamed: 0',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqzwV23chtKw"
      },
      "source": [
        "target = data['target']\n",
        "data = data.drop('target',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeJbMrA8hCqT",
        "outputId": "773f4f68-54a1-4f8c-dea0-21d9e977f0fc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.3, random_state = 77)\n",
        "print (\"X_train: \", len(X_train))\n",
        "print(\"X_test: \", len(X_test))\n",
        "print(\"y_train: \", len(y_train))\n",
        "print(\"y_test: \", len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  28000\n",
            "X_test:  12000\n",
            "y_train:  28000\n",
            "y_test:  12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU2dDj-XqP2z",
        "outputId": "c0411afb-4e18-48d7-930a-a05ddcd245f7"
      },
      "source": [
        "# Gridsearch \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid={\"C\":np.logspace(-4,-1,4)}\n",
        "# Add \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"]\n",
        "log_reg = LogisticRegression(random_state=77,max_iter = 10000)\n",
        "log_reg_cv = GridSearchCV(log_reg,grid,cv=5)\n",
        "log_reg_cv.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=10000, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=77, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': array([0.0001, 0.001 , 0.01  , 0.1   ])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCD-LhPv6rnP",
        "outputId": "fddc40d3-50e7-4f7b-f7e8-a37f6920343d"
      },
      "source": [
        "print(\"Best Hyperparameter: \",log_reg_cv.best_params_)\n",
        "print(\"Accuracy: \",log_reg_cv.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Hyperparameter:  {'C': 0.1}\n",
            "Accuracy:  0.9026071428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D9qrTmet4UG",
        "outputId": "de33351b-91ee-4e71-e624-23b9700ca49e"
      },
      "source": [
        "grid1={\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"]}\n",
        "log_reg1 = LogisticRegression(C=0.1,random_state=77,max_iter = 10000)\n",
        "log_reg_cv1 = GridSearchCV(log_reg1,grid1,cv=5)\n",
        "log_reg_cv1.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=10000, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=77, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bryKYbGc6b9F",
        "outputId": "1b516275-0990-42ec-c748-04ea4454341d"
      },
      "source": [
        "print(\"Best Hyperparameter: \",log_reg_cv1.best_params_)\n",
        "print(\"Accuracy: \",log_reg_cv1.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Hyperparameter:  {'solver': 'liblinear'}\n",
            "Accuracy:  0.9041428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YE03y3SeGhn"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf = LogisticRegressionCV(cv=5, random_state=77, max_iter=10000).fit(X_train, y_train)\n",
        "y_locv = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct0AsNYV9XLr",
        "outputId": "7d24b9f7-f26c-49fb-d85a-f9dfc0042c20"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "ms3VdWyNeUQe",
        "outputId": "81fc2094-e59d-465a-eb0e-58edea0016a0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "print('Logistic Regression accuracy score: ',accuracy_score(y_test,y_locv))\n",
        "print('Logistic Regression matrix heatmap: \\n')\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(confusion_matrix(y_locv, y_test),annot=True, annot_kws={\"size\": 16},fmt = \"d\")\n",
        "plt.show()\n",
        "print('Logistic Regression classification report \\n',classification_report(y_test, y_locv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression accuracy score:  0.90425\n",
            "Logistic Regression matrix heatmap: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression classification report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91      5967\n",
            "           1       0.95      0.85      0.90      6033\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.91      0.90      0.90     12000\n",
            "weighted avg       0.91      0.90      0.90     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLfSrRzh86i"
      },
      "source": [
        "# Without CV -> Vanilla LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lo = LogisticRegression(max_iter=10000,C=0.1,solver='liblinear')\n",
        "lo.fit(X_train,y_train)\n",
        "y_lo = lo.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "nemNXK6tiKAL",
        "outputId": "253f7d86-b62c-48b2-ddce-f3505c7b706f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "train_scores, valid_scores = validation_curve(LogisticRegression(max_iter=10000), X_train, y_train,\"C\",np.logspace(-7, 3, 3),cv=5)\n",
        "train_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b615fe254ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[0;34m(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose, error_score)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         error_score=error_score)\n\u001b[1;32m   1471\u001b[0m         \u001b[0;31m# NOTE do not change order of iteration to allow one time cv splitters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         for train, test in cv.split(X, y, groups) for v in param_range)\n\u001b[0m\u001b[1;32m   1473\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0mn_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             )\n\u001b[1;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_intercept_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_intercept_dot\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "zWH-qwHLiNSe",
        "outputId": "91a86a4f-8551-455d-bc74-4b5fd7b683dd"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "print('Logistic Regression accuracy score: ',accuracy_score(y_test,y_lo))\n",
        "print('Logistic Regression matrix heatmap: \\n')\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(confusion_matrix(y_lo, y_test),annot=True, annot_kws={\"size\": 16},fmt = \"d\")\n",
        "plt.show()\n",
        "print('Logistic Regression classification report \\n',classification_report(y_test, y_lo))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression accuracy score:  0.9043333333333333\n",
            "Logistic Regression matrix heatmap: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression classification report \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      5967\n",
            "           1       0.95      0.85      0.90      6033\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.91      0.90      0.90     12000\n",
            "weighted avg       0.91      0.90      0.90     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3vFY7_XB-oM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}